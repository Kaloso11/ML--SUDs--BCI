#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Mar  2 13:53:56 2021

@author: kaloso
"""

import random 
from numpy import unique
from numpy import where
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt 
from sklearn.preprocessing import StandardScaler
from IPython.display import display
from sklearn.cluster import KMeans 
from sklearn.decomposition import PCA
from mpl_toolkits.mplot3d import Axes3D
from sklearn.metrics import homogeneity_score, completeness_score, \
v_measure_score, adjusted_rand_score, adjusted_mutual_info_score, silhouette_score
import seaborn as sns
from sklearn import preprocessing
import matplotlib.pyplot as plt
import matplotlib.cbook as cbook
from matplotlib import style
from numpy import asarray
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.datasets import make_classification
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
from sklearn.metrics import pairwise_distances_argmin
from scipy.stats import chi2_contingency
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import confusion_matrix,zero_one_loss
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn import utils
import warnings
from sklearn.metrics import confusion_matrix,classification_report
from mpl_toolkits.mplot3d import Axes3D
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as shc
from sklearn.preprocessing import normalize
from sklearn.cluster import OPTICS, cluster_optics_dbscan
import matplotlib.gridspec as gridspec
float_formatter = lambda x: "%.3f" % x
np.set_printoptions(formatter={'float_kind':float_formatter})
from sklearn.cluster import SpectralClustering, KMeans
from sklearn.metrics import pairwise_distances
from matplotlib import pyplot as plt
import networkx as nx
from pandas.plotting import scatter_matrix
from scipy.cluster.hierarchy import linkage, fcluster
from sklearn import preprocessing, decomposition
from matplotlib.collections import LineCollection
import time
import operator
from scipy.spatial.distance import pdist
from scipy.spatial.distance import squareform
from sklearn.cluster import DBSCAN
from scipy.cluster.hierarchy import dendrogram
from sklearn.neighbors import NearestNeighbors
from sklearn.impute import KNNImputer
from sklearn.manifold import TSNE
import math
from sklearn import svm
from sklearn.model_selection import KFold
from sklearn.svm import SVR
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
#from functions import *
sns.set()
warnings.filterwarnings('ignore')


alcohol = pd.read_excel("Documents/MEng Research/NSDUH/2019 Data/DATA-Alcohol.xlsx")
marijuana = pd.read_excel("Documents/MEng Research/NSDUH/2019 Data/DATA-Marijuana.xlsx")
cocaine = pd.read_excel("Documents/MEng Research/NSDUH/2019 Data/DATA-Cocaine.xlsx")
stimulants = pd.read_excel("Documents/MEng Research/NSDUH/2019 Data/DATA-Stimulants.xlsx")
sedatives = pd.read_excel("Documents/MEng Research/NSDUH/2019 Data/DATA-Sedatives.xlsx")
#sns.heatmap(marijuana.isnull(),yticklabels=False, cbar=False,cmap='Blues')

#print(alcohol.isnull().sum())

#print(df.head())


alcohol.loc[(alcohol['GRSKBNGDLY'].isnull()==True),'GRSKBNGDLY'] = alcohol['GRSKBNGDLY'].mean()
alcohol.loc[(alcohol['GRSKBNGWK'].isnull()==True),'GRSKBNGWK'] = alcohol['GRSKBNGWK'].mean()
marijuana.loc[(marijuana['GRSKMRJMON'].isnull()==True),'GRSKMRJMON'] = marijuana['GRSKMRJMON'].mean()
marijuana.loc[(marijuana['GRSKMRJWK'].isnull()==True),'GRSKMRJWK'] = marijuana['GRSKMRJWK'].mean()
marijuana.loc[(marijuana['DIFOBTMRJ'].isnull()==True),'DIFOBTMRJ'] = marijuana['DIFOBTMRJ'].mean()
cocaine.loc[(cocaine['GRSKCOCMON'].isnull()==True),'GRSKCOCMON'] = cocaine['GRSKCOCMON'].mean()
cocaine.loc[(cocaine['GRSKCOCWK'].isnull()==True),'GRSKCOCWK'] = cocaine['GRSKCOCWK'].mean()



#print(marijuana.isnull().sum())
#print(cocaine.isnull().sum())
#print(alcohol.duplicated().sum())

df = cocaine.iloc[0:25000,:]


#print(df.describe())



data_scale = normalize(df)
df_scaled = pd.DataFrame(data_scale, columns=df.columns)

scaler = StandardScaler()
data = scaler.fit_transform(df)

#DATA IMPUTATION USING KNN

# imputer = KNNImputer(n_neighbors=5)

# df_impute = imputer.fit_transform(alcohol)
#print(df_impute.isnull().sum())

# factor = 3
# upper_lim = data['column'].mean () + data['column'].std () * factor
# lower_lim = data['column'].mean () - data['column'].std () * factor

# outliers = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]

# print(outliers)

#HEATMAP

# corr = df_scaled.corr(method='spearman')
# f,ax = plt.subplots(figsize=(12,9))
# cmap = sns.diverging_palette(10, 275,as_cmap=True )
# sns.heatmap(corr, cmap=cmap,square=True,linewidths=1,cbar_kws={'shrink':0.5},ax=ax)

#ELBOW METHOD

ks = range(1, 10)
inertias = []

for k in ks:
    model = KMeans(n_clusters=k)
    model.fit(data)
    inertias.append(model.inertia_)

plt.figure(figsize=(8,5))
plt.style.use('bmh')
plt.plot(ks, inertias, '-o')
plt.xlabel('Number of clusters, k')
plt.ylabel('Inertia')
plt.xticks(ks)
plt.show()

pca = PCA(n_components=2,random_state=123)
principal_comp = pca.fit_transform(data)
pca_df = pd.DataFrame(data=principal_comp,columns=['pca1','pca2'])

#print(pca.explained_variance_ratio_)

def k_means(n_clust, data_frame):

    k_means = KMeans(n_clusters = n_clust, random_state=123, n_init=30)
    global me
    me=k_means.fit(data_frame)
    global c_labels
    c_labels = k_means.labels_
    y_clust = k_means.predict(data_frame)
    global k_centers
    k_centers = k_means.cluster_centers_
    
k_cluster=k_means(n_clust=2, data_frame=pca_df)

arr_k = np.unique(c_labels)
#print(arr_k) 
#print(c_labels[:1000])
pca_dfk2 = pd.concat([pca_df,pd.DataFrame({'cluster':c_labels})],axis=1)

fig = plt.figure(figsize=(12,8))
ax = sns.scatterplot(x="pca1",y="pca2",hue='cluster',data=pca_dfk2,palette=['red','green'])#,'blue','yellow','purple','black','pink','orange','maroon'])
plt.show()



#HIERARCHICAL CLUSTERING


def Agglomerative(n_clust, data_frame):


    hierarchy = AgglomerativeClustering(n_clusters = n_clust, affinity='euclidean', linkage='ward',compute_full_tree=True)
    global hie
    hie=hierarchy.fit(data_frame)
    global h_labels
    h_labels = hierarchy.labels_
    #h_clust = hierarchy.predict(data_frame)
    
h_cluster = Agglomerative(n_clust=5, data_frame=pca_df)
arr_h = np.unique(h_labels)
#print(arr_h)
pca_dfh2 = pd.concat([pca_df,pd.DataFrame({'cluster':h_labels})],axis=1)

# fig = plt.figure(figsize=(12,8))
# ax = sns.scatterplot(x="pca1",y="pca2",hue='cluster',data=pca_dfh2,palette=['red','green','blue','yellow','purple'])#,'black','pink','orange','maroon'])
# plt.show()


#DBSCAN


# neighbors = NearestNeighbors(n_neighbors=20)
# neighbors_fit = neighbors.fit(pca_df)
# distances, indices = neighbors_fit.kneighbors(pca_df)
# distances = np.sort(distances, axis=0)
# distances = distances[:,1]
# plt.plot(distances)

def Dbscan(eps,min_samples,data_frame):

    dbscan = DBSCAN(metric='euclidean')
    global db
    db=dbscan.fit(data_frame)
    global d_labels
    d_labels = dbscan.labels_
    #d_clust = dbscan.predict(data_frame)
    
d_cluster = Dbscan(eps=0.5,min_samples=5,data_frame=pca_df)
arr_d = np.unique(d_labels)
#print(arr_d)

#print (d_labels[:1000])


pca_dfd2 = pd.concat([pca_df,pd.DataFrame({'cluster':d_labels})],axis=1)
# fig = plt.figure(figsize=(12,8))
# ax = sns.scatterplot(x="pca1",y="pca2",hue='cluster',data=pca_dfd2,palette=['red','green','blue','yellow'])#,'purple','black','pink','orange','maroon'])
# plt.show()


colours = {} 
colours[0] = 'r'
colours[1] = 'g'
colours[2] = 'b'
colours[3] = 'k'
colours[-1] = 'y'

  
# Building the colour vector for each data point 
#cvec = [colours[label] for label in d_labels] 
  
# For the construction of the legend of the plot 
# r = plt.scatter(pca_df2['pca1'], pca_df2['pca2'], c ='r'); 
# g = plt.scatter(pca_df2['pca1'], pca_df2['pca2'], c ='g'); 
# b = plt.scatter(pca_df2['pca1'], pca_df2['pca2'], c ='b'); 
# k = plt.scatter(pca_df2['pca1'], pca_df2['pca2'], c ='k');
# y = plt.scatter(pca_df2['pca1'], pca_df2['pca2'], c ='y'); 

  
# # Plotting pca1 on the X-Axis and pca2 on the Y-Axis  

# plt.figure(figsize =(12, 12)) 
# plt.scatter(pca_dfd2['pca1'], pca_dfd2['pca2'], c = cvec) 
  
# # Building the legend 
# plt.legend((r, g, b, k, y), ('Label 0', 'Label 1', 'Label 2','Label 3', 'Label -1')) 
  
# plt.show() 


#OPTICS

optics_model = OPTICS(min_samples = 10, xi = 0.05, min_cluster_size = 0.05) 
optics_model.fit(pca_df)

# Producing the labels according to the DBSCAN technique with eps = 0.5 
labels1 = cluster_optics_dbscan(reachability = optics_model.reachability_, 
                                   core_distances = optics_model.core_distances_, 
                                   ordering = optics_model.ordering_, eps = 0.5)

# Producing the labels according to the DBSCAN technique with eps = 2.0 
labels2 = cluster_optics_dbscan(reachability = optics_model.reachability_, 
                                   core_distances = optics_model.core_distances_, 
                                   ordering = optics_model.ordering_, eps = 2) 

# Creating a numpy array with numbers at equal spaces till the specified range 
space = np.arange(len(pca_df))

# Storing the reachability distance of each point 
reachability = optics_model.reachability_[optics_model.ordering_]

# Storing the cluster labels of each point 
labels = optics_model.labels_[optics_model.ordering_]

#print(labels[:5000])
#print(np.unique(labels))



def Optics(eps,min_samples,data_frame):

    optics = OPTICS(xi = 0.05, min_cluster_size = 0.05)
    global op
    op=optics.fit(data_frame)
    global o_labels
    o_labels = optics.labels_
    #d_clust = dbscan.predict(data_frame)
    
o_cluster = Optics(eps=0.5,min_samples=5,data_frame=pca_df)
arr_o = np.unique(o_labels)
#print(arr_o)

pca_dfo2 = pd.concat([pca_df,pd.DataFrame({'cluster':o_labels})],axis=1)
# fig = plt.figure(figsize=(12,8))
# ax = sns.scatterplot(x="pca1",y="pca2",hue='cluster',data=pca_dfo2,palette=['red','green','blue','yellow'])#,'purple','black','pink','orange','maroon'])
# plt.show()


#K FOLD CROSS VALIDATION

#model_train, model_test, cluster_train, cluster_test = train_test_split(pca_df,d_labels,test_size=0.4,random_state=0)
#print(model_train.shape)

# clf = svm.SVC(kernel='linear',C=1).fit(model_train,cluster_train)
#print(clf.score(model_test,cluster_test))



# scores = []
# best_svr = SVR(kernel='rbf')
# cv = KFold(n_splits=10, random_state=42, shuffle=True)
# for train_index, test_index in cv.split(pca_df):
#     print("Train Index: ", train_index, "\n")
#     print("Test Index: ", test_index)


#model_train, model_test, cluster_train, cluster_test = pca_df[train_index],pca_df[test_index],c_labels[train_index],c_labels[test_index]
# best_svr.fit(model_train,cluster_train)
# scores.append(best_svr.score(model_test,cluster_test))

#print(np.mean(scores))

#print(cross_val_score(best_svr, pca_df, c_labels, cv=10))

#Importing required libraries

 
#Implementing cross validation
X = pca_dfh2.iloc[:,:-1]
y = pca_dfh2.iloc[:,-1]
  
#SILHOUTTE SCORE
score = silhouette_score(X,y,metric='euclidean')
print('Silhoutte:%3f' %score)

k = 10
kf = KFold(n_splits=k, random_state=None)
model2 = LogisticRegression(multi_class='multinomial',solver= 'lbfgs')

acc_score = []
 
for train_index , test_index in kf.split(X):
    #model_train, model_test, cluster_train, cluster_test = data[train_index],data[test_index],c_labels[train_index],c_labels[test_index]
    X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]
    y_train , y_test = y[train_index] , y[test_index]
     
    model2.fit(X_train,y_train)
    pred_values = model2.predict(X_test)
     
    acc = accuracy_score(pred_values , y_test)
    acc_score.append(acc)
     
avg_acc_score = sum(acc_score)/k
 
# print('accuracy of each fold - {}'.format(acc_score))
# print('Avg accuracy : {}'.format(avg_acc_score))

mape = mean_absolute_error(y_test,pred_values)*100
print(mape)

rmse = mean_squared_error(y_test,pred_values)
print(math.sqrt(rmse))