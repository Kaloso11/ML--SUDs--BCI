#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Aug  9 19:10:32 2020

@author: kaloso
"""
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.cbook as cbook
from matplotlib import style
from numpy import asarray
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.datasets import make_classification
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
from sklearn.metrics import pairwise_distances_argmin
from scipy.stats import chi2_contingency
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import confusion_matrix,zero_one_loss
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn import utils
import warnings

warnings.filterwarnings('ignore')
dataset = pd.read_csv("Documents/MEng Research/NSDUH/NSDUH_2018_Tab.tsv")

df = pd.read_csv("Documents/MEng Research/A Survey on Addiction (Responses) - Original2.csv",sep=",")
df.drop('Timestamp',axis=1,inplace=True)
df.drop('Sex',axis=1,inplace=True)


#print(df.iloc[:,:])

df['Q1'].replace (['Everyday','Once week','Twice a week','Twice a day'],[3,1,2,4],inplace=True)
df['Q2'].replace (['Yes','No','Not really'],[2,1,1.5],inplace=True)
df['Q3'].replace (['Yes','No','Not sure'],[2,1,1.5],inplace=True)
df['Q4'].replace (['Does not matter','More than three','Two to three','Alone'],[1,2,3,4],inplace=True)
df['Q5'].replace (['Many','10 more','Five more','Three more'],[1,1.5,2,2.5],inplace=True)
df['Q6'].replace (['Many','Around 10','Three to Five','One or two'],[1,1.5,2,2.5],inplace=True)
df['Q7'].replace (['Yes','No','Not really'],[2,1,1.5],inplace=True)
df['Q8'].replace (['Yes','No','Manageable'],[2,1,1.5],inplace=True)
df['Q9'].replace (['No, I keep it to myself','Close friends and family only','Selected few','Everybody knows about it'],[2.5,2,1.5,1],inplace=True)
df['AD'].replace (['Manageable','Addicted','Not addicted',"I don't know"],[2,3,0,1],inplace=True)


# corr = df.corr(method='spearman')
# f,ax = plt.subplots(figsize=(12,9))
# cmap = sns.diverging_palette(10, 275,as_cmap=True )

# sns.heatmap(corr, cmap=cmap,square=True,linewidths=1,cbar_kws={'shrink':0.5},ax=ax)  

data_corr = df.corr(method='spearman')    #Numeric data correlation

x = df.iloc[:,0:-1]
y = df.iloc[:,-1]


#x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.20,random_state = 4) #train-test split

# FEATURE SCALING

#x_train = sc.fit_transform(x_train)
#x_test = sc.transform(x_test)


# norm = MinMaxScaler().fit(sc)
# sc2 = norm.transform(sc)
# x_test = norm.transform(x_test)

# sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='Blues')

# print(df.isnull().sum()) 

# # PROBABILITY DENSITY
# plt.figure(figsize=(10,50))

# for i in range(len(x.columns)):
#     plt.subplot(17,1,i+1)
#     sns.distplot(df[x.columns[i]],kde_kws={'color':'b','lw':3,'label':'KDE'},hist_kws={'color':'g'})
#     plt.title(x.columns[i])

# plt.tight_layout()

# #FEATURE CORRELATION
# corr = x.corr()
# f, ax = plt.subplots(figsize=(10,10))
# sns.heatmap(corr,annot=True)

#FEATURE SCALING
scaler = StandardScaler()
sc = scaler.fit_transform(x)


# ELBOW METHOD TO FIND OPTIMAL NUMBER OF CLUSTERS

scores = []

range_values = range(1,20)

for i in range_values:
    kmeans = KMeans(n_clusters= i)
    kmeans.fit(sc)
    scores.append(kmeans.inertia_)
    
plt.plot(scores,'bx-')

kmeans = KMeans(7)
kmeans.fit(sc)
labels = kmeans.labels_

clusters = pd.DataFrame(data=kmeans.cluster_centers_,columns=[x.columns])
#print(clusters)

cluster_centres = scaler.inverse_transform(clusters)
cluster_centers = pd.DataFrame(data=cluster_centres,columns=[x.columns]) 
#print(cluster_centers)


y_kmeans =kmeans.fit_predict(sc)
#print(y_kmeans)

df_cluster = pd.concat([x,pd.DataFrame({'cluster':labels})],axis=1)
#print(df_cluster.head(10))

#BAR GRAPHS OF CLUSTERS

# for i in x.columns:
#     plt.figure(figsize=(30,5))
#     for j in range(7):
#         plt.subplot(1,7,j+1)
#         cluster = df_cluster[df_cluster['cluster']==j]
#         cluster[i].hist(bins=20)
#         plt.title('{}'.format(i,j))
        
        
#     plt.show()    

#PRINCIPAL COMPONENT ANALYSIS

pca = PCA(n_components=2)
principal_comp = pca.fit_transform(sc)
#print(principal_comp)

pca_df = pd.DataFrame(data=principal_comp,columns=['pca1','pca2'])

pca_df = pd.concat([pca_df,pd.DataFrame({'cluster':labels})],axis=1)
print(pca_df.head())

plt.figure(figsize=(10,10))
ax = sns.scatterplot(x="pca1",y="pca2",hue='cluster',data=pca_df,palette=['red','green','blue','yellow','purple','black','orange','pink'])
plt.show()

print(pca.explained_variance_ratio_)
# Plot the explained variances
# features = range(pca.n_components_)
# plt.bar(features, pca.explained_variance_ratio_, color='black')
# plt.xlabel('PCA features')
# plt.ylabel('variance %')
# plt.xticks(features)

cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')
cluster.fit_predict(df)

plt.figure(figsize=(10, 7))
plt.title("Customer Dendograms")
dend = shc.dendrogram(shc.linkage(df_cluster, method='ward'))


clust = OPTICS(min_samples=50, xi=.05, min_cluster_size=.05)
clust.fit(X)

labels_050 = cluster_optics_dbscan(reachability=clust.reachability_,
                                   core_distances=clust.core_distances_,
                                   ordering=clust.ordering_, eps=0.5)
labels_200 = cluster_optics_dbscan(reachability=clust.reachability_,
                                   core_distances=clust.core_distances_,
                                   ordering=clust.ordering_, eps=2)

space = np.arange(len(df))
reachability = clust.reachability_[clust.ordering_]
labels = clust.labels_[clust.ordering_]

plt.figure(figsize=(10, 7))
G = gridspec.GridSpec(2, 3)
ax1 = plt.subplot(G[0, :])
ax2 = plt.subplot(G[1, 0])
ax3 = plt.subplot(G[1, 1])
ax4 = plt.subplot(G[1, 2])

# Reachability plot
colors = ['g.', 'r.', 'b.', 'y.', 'c.']
for klass, color in zip(range(0, 5), colors):
    Xk = space[labels == klass]
    Rk = reachability[labels == klass]
    ax1.plot(Xk, Rk, color, alpha=0.3)
ax1.plot(space[labels == -1], reachability[labels == -1], 'k.', alpha=0.3)
ax1.plot(space, np.full_like(space, 2., dtype=float), 'k-', alpha=0.5)
ax1.plot(space, np.full_like(space, 0.5, dtype=float), 'k-.', alpha=0.5)
ax1.set_ylabel('Reachability (epsilon distance)')
ax1.set_title('Reachability Plot')

# # OPTICS
# colors = ['g.', 'r.', 'b.', 'y.', 'c.']
# for klass, color in zip(range(0, 5), colors):
#     Xk = X[clust.labels_ == klass]
#     ax2.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)
# ax2.plot(X[clust.labels_ == -1, 0], X[clust.labels_ == -1, 1], 'k+', alpha=0.1)
# ax2.set_title('Automatic Clustering\nOPTICS')

# # DBSCAN at 0.5
# colors = ['g', 'greenyellow', 'olive', 'r', 'b', 'c']
# for klass, color in zip(range(0, 6), colors):
#     Xk = X[labels_050 == klass]
#     ax3.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3, marker='.')
# ax3.plot(X[labels_050 == -1, 0], X[labels_050 == -1, 1], 'k+', alpha=0.1)
# ax3.set_title('Clustering at 0.5 epsilon cut\nDBSCAN')

# # DBSCAN at 2.
# colors = ['g.', 'm.', 'y.', 'c.']
# for klass, color in zip(range(0, 4), colors):
#     Xk = X[labels_200 == klass]
#     ax4.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)
# ax4.plot(X[labels_200 == -1, 0], X[labels_200 == -1, 1], 'k+', alpha=0.1)
# ax4.set_title('Clustering at 2.0 epsilon cut\nDBSCAN')

# plt.tight_layout()
# plt.show()
